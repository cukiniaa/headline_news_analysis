"""news_classification.ipynb

Automatically generated by Colaboratory.
"""

import json
import pandas as pd

# TODO download data from the website
rows = []
filename = 'sample_data/News_Category_Dataset_v2.json'
with open(filename) as f:
    for line in f:
        rows.append(json.loads(line))

data = pd.DataFrame(rows)

data = data.drop(columns=['authors', 'link', 'short_description', 'date'])
data['category'] = data['category'].astype('category')

data['category'].value_counts().plot.barh(figsize=(10,10))
print(data.info())

# Merge similar categories

new_categories = {
    'ENVIRONMENT': ['GREEN', 'ENVIRONMENT'],
    'GROUPS VOICES': ['LATINO VOICES', 'BLACK VOICES', 'QUEER VOICES'],
    'CULTURE': ['CULTURE & ARTS', 'ARTS & CULTURE', 'ARTS', 'RELIGION'],
    'FINANCE': ['BUSINESS', 'MONEY'],
    'SCIENCE & TECH': ['SCIENCE', 'TECH'],
    'EDUCATION': ['EDUCATION', 'COLLEGE'],
    'WORLD NEWS': ['WORLD NEWS', 'THE WORLDPOST' , 'WORLDPOST'],
    'ENTERTAINMENT': ['ENTERTAINMENT', 'COMEDY'],
    'WELLNESS': ['HEALTHY LIVING', 'WELLNESS'],
    'FOOD & DRINK': ['FOOD & DRINK', 'TASTE'],
    'STYLE & BEAUTY': ['STYLE & BEAUTY', 'STYLE'],
    'RELATIONSHIPS': ['PARENTING', 'PARENTS', 'WEDDINGS', 'DIVORCE'],
    'OTHER': ['GOOD NEWS', 'WEIRD NEWS', 'FIFTY']
}

mapper = {}
for key, values in new_categories.items():
    for val in values:
        mapper[val] = key

data = data.replace({'category': mapper})
data['category'] = data['category'].astype('category')

data['category'].value_counts().plot.barh(figsize=(10,10))
print("Number of all headlines %d." % len(data))

import nltk
import re
import numpy as np

nltk.download('wordnet')
nltk.download('stopwords')

from nltk.corpus import stopwords
from nltk.stem.wordnet import WordNetLemmatizer

lem = WordNetLemmatizer()

def preprocess(text):
    text = re.sub("U.S.", "USA", text)
    text = re.sub("[^a-z ]", " ", text.lower())
    text = re.sub("photos?|videos?", "", text)
    words = text.split()
    words = [lem.lemmatize(w) for w in words]
    words = [w for w in words if len(w) > 2 and w not in stopwords.words("english")]
    return words

data['headline'] = data['headline'].apply(preprocess)
print(data)

from collections import Counter
from pandas.core.common import flatten

grouped = data.groupby(by='category', as_index=False).agg(lambda a: list(flatten(a)))
grouped['headline'] = [Counter(h_a) for h_a in  grouped['headline']]
grouped['common_words'] = [sorted(group.items(), key=lambda x: x[1], reverse=True)[:10] for group in grouped['headline']]

for _, row in grouped.iterrows():
    print(row['category'], row['common_words'])

from sklearn.utils import resample

value_counts = data['category'].value_counts()
sorted_keys = value_counts.keys()
rows_per_cat = value_counts[sorted_keys[0]]

data_upsampled = pd.DataFrame(data[data['category'] == sorted_keys[0]])

for key in sorted_keys[1:]:
    resampled = resample(data[data['category'] == key],
                         replace=True,
                         n_samples=rows_per_cat)
    data_upsampled = pd.concat([data_upsampled, resampled])

data = data_upsampled

# Bag of words
from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfTransformer

# TODO what about HashingVectorizer
# TODO add tokenizer and preprocessor here
vectorizer = CountVectorizer(
    tokenizer=lambda vec: vec,
    preprocessor=lambda vec: vec,
    ngram_range=(1, 2),
    max_df=8940, # decided after analyzing the overall frequency of tokens
    max_features=50000)
X = vectorizer.fit_transform(data['headline'])
print("Number of tokens:", len(vectorizer.get_feature_names()))

# transformer = TfidfTransformer(smooth_idf=False)
# X = transformer.fit_transform(X)
y = data['category'].cat.codes
cat_dict = dict(enumerate(data['category'].cat.categories)) # dict { code: label }

print("Shape of X:", X.shape)

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)

d = dict(zip(vectorizer.get_feature_names(), np.array(X.sum(axis=0)).flatten()))
sorted(d.items(), key=lambda kv: kv[1], reverse=False)

from sklearn import metrics
from time import time

def test_classifier(clf):
    train_t0 = time()
    clf.fit(X_train, y_train)
    train_time = time() - train_t0

    pred_t0 = time()
    pred = clf.predict(X_test)
    pred_time = time() - pred_t0

    acc = metrics.accuracy_score(y_test, pred)
    prec = metrics.precision_score(y_test, pred, average='weighted')
    rec = metrics.recall_score(y_test, pred, average='weighted')
    f1 = metrics.f1_score(y_test, pred, average='weighted')

    print(clf.__class__.__name__)
    print("SCORES Accuracy: %.3f. Precision: %.3f. Recall: %.3f. F1: %.3f." % (acc, prec, rec ,f1))
    print("TRAINING TIME %.3f s." % train_time)
    print("PREDICTION TIME %.3f s." % pred_time)
    print("-------------------------------")

    return pred

def binary_class_precision(y_test, pred, codes_dict, print_scores=False):
    precision = {}
    for code, label in codes_dict.items():
        bin_y_test = y_test == code
        bin_pred = pred == code
        precision[label] = metrics.precision_score(bin_y_test, bin_pred)
        if print_scores:
            print("%s %.5f" % (label, precision[label]))
    return precision

from sklearn.svm import LinearSVC
from sklearn.naive_bayes import BernoulliNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import SGDClassifier

clf = LinearSVC()
pred = test_classifier(clf)

codes_dict = dict(enumerate(data['category'].cat.categories))
binary_class_precision(y_test, pred, codes_dict, print_scores=True)
print("-------------------------------")

from sklearn.decomposition import TruncatedSVD 
import seaborn as sns
import matplotlib.pyplot as plt

pca = TruncatedSVD(n_components=2)
X_pca = pca.fit(X)
X_pca_scores = X_pca.transform(X)

plt.figure(figsize=(10, 10))
sns.scatterplot(x=X_pca_scores[:, 0], y=X_pca_scores[:, 1], hue=data['category'])

# TODO Only important tokens:

# 1) use the vocabulary feature in CountVectorized: a dict, where keys are 
#    terms and values are indices in the feature matrix, or an iterable over
#    terms
# 2) use max_features/min_df/max_df in CountVectorized
# 3) use TfidfTransformer
